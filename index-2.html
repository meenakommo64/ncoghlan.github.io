<!DOCTYPE html><html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta charset="utf-8">
    <meta name="description" content="Efficiency (a virtue) is the child of laziness and greed (both vices), while
much of our economic activity is devoted to preventing boredom in the idle
time created by increases in efficiency. To be human is to be a strange
creature indeed :)
">
    <meta name="author" content="Nick Coghlan">
    <title>Curious Efficiency (old posts page 2) | Curious Efficiency</title>
    
            <link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
            <link href="assets/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css">
        <link href="assets/css/rst.css" rel="stylesheet" type="text/css">
        <link href="assets/css/code.css" rel="stylesheet" type="text/css">
        <link href="assets/css/colorbox.css" rel="stylesheet" type="text/css">
        <link href="assets/css/theme.css" rel="stylesheet" type="text/css">
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js" type="text/javascript"></script>
    <![endif]-->
            <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">

    
    
    
</head>
<body>
<!-- Menubar -->
<div class="navbar navbar-fixed-top">
    <div class="navbar-inner">
        <div class="container">

        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
        </a>

            <a class="brand" href=".">
            Curious Efficiency
            </a>
            <!-- Everything you want hidden at 940px or less, place within here -->
            <div class="nav-collapse collapse">
                <ul class="nav">
                    
            <li><a href="archive.html">Archives</a>
            </li><li><a href="categories/index.html">Tags</a>
            </li><li><a href="rss.xml">RSS</a>

                </li></ul>
                <ul class="nav pull-right">
                
                
                 
                </ul>
            </div>
        </div>
    </div>
</div>
<!-- End of Menubar -->
<div class="container-fluid" id="container-fluid">
    <!--Body content-->
    <div class="row-fluid">
    <div class="span2"></div>
    <div class="span8">
    
        <div class="postbox">
        <h1><a href="posts/201108of-python-and-road-maps-or-lack-thereof.html">Of Python and Road Maps (or the lack thereof)</a>
        <small>  
             Posted: <time class="published" datetime="2011-08-24T05:16:00">2011-08-24 05:16</time>
        </small></h1>
        <hr>
        <p>I gave my <a href="http://www.youtube.com/watch?v=UZIq_9KgFEY">first ever full conference talk</a> at PyCon AU this year, on the topic of how Python evolves over time.<br><br>One notable absence from that talk is a discussion of any kind of specific 'road map' for the language as a whole. There's a reason for that: no such document exists. Guido van Rossum (as the language creator and Benevolent Dictator for Life) is the only person who could write an authoritative one and he's generally been happy to sit back and <a href="http://mail.python.org/pipermail/python-dev/2011-March/108584.html">let things evolve organically</a> since guiding the initial phases of the Py3k transition (which is now ticking along quite nicely and the outlook for 2.x being relegated to 'supported-but-now-legacy' status within the 2013/14 time frame is promising). <a href="http://www.python.org/dev/peps/pep-0398/">PEP 398</a>, the release PEP for Python 3.3 comes close to qualifying, but is really just a list of PEPs and features, with no context for why those ideas are even on the list.<br><br>However, despite that, it seems worthwhile to make the effort to try to write a <i>descriptive</i> road map for where I personally see current efforts going, and where I think the Python core has a useful role to play. Looking at the <a href="http://www.python.org/dev/peps/">Python Enhancement Proposal index</a> for the lists of Accepted and Open PEPs can be informative, but it requires an experienced eye to know which proposals are currently being actively championed and have a decent chance of being accepted and included in a CPython release.<br><br>When reading the following, please keep in mind that the overall Python community is much bigger than the core development team, so there are plenty of other things going on out there. Others may also have differing opinions on which changes are most likely to come to fruition within the next few years.<br><br></p><h1>Always in Motion is the Future</h1><h2>Unicode is coming. Are you ready?</h2>The Python 2.x series lets many developers sit comfortably in a pure ASCII world until the day when they have to debug a bizarre error caused by their datastream being polluted with non-ASCII data.<br><br>That is only going to be a comfortable place to be for so long. More and more components of the computing environment are migrating from ASCII to Unicode, including filesystems, some network protocols, user interface elements, other languages and various other operating system interfaces. With the modern ease of international travel and the rise of the internet, even comfortable "we only speak ASCII in this office, dammit!" environments are eventually going to have to face employees and customers that want to use their own names in their own native, non-Latin character sets rather than being constrained by a protocol invented specifically to handle English.<br><br>The Python 3.x series was created largely in response to that environmental transition - in many parts of the world where English isn't the primary spoken language, needing to deal with text in multiple disparate character encodings is already the norm, and having to deal regularly with non-ASCII data is only going to become more common even in English dominant areas.<br><br>However, supporting Unicode correctly on this scale is something of a learning experience for the core development team as well. Several parts of the standard library (most notably the email package and wsgiref module) really only adjusted to the new Unicode order of things in the recent 3.2 release, and (in my opinion) we've already made at least one design mistake in leaving various methods on bytes and bytearray objects that assume the underlying data is text encoded as ASCII.<br><br>The conversion to Unicode everywhere also came at a measurable cost in reduced speed and increased memory usage within the CPython interpreter. <a href="http://www.python.org/dev/peps/pep-0393/">PEP 393</a> ("Flexible String Representation") is an attempt to recover some of that ground by using a smarter internal representation for string objects.<br><br><h2>Easier and more reliable distribution of Python code</h2>Tools to package and distribute Python code have been around in various stages of usability since before the creation of distutils. Currently, there is a concerted effort by the developers of tools like pip, virtualenv, distutils2, distribute and the Python Package index to make it easier to create and distribute packages, to provide better metadata about packages (without requiring execution of arbitrary Python code), to install packages published by others, and to play nicely with distribution level package management techniques.<br><br>The new 'packaging' module that will arrive in 3.3 (and will be backported to 3.2 and 2.x under the 'distutils2' name) is a visible sign of that, as are several Python Enhancement Proposals related to packaging metadata standards. These days, the CPython interpreter also supports direct execution of code located in <a href="http://www.boredomandlaziness.org/2011/03/what-is-python-script.html">a variety of ways</a>.<br><br>There's a significant social and educational component to this effort in addition to the technical underpinnings, so this is going to take some time to resolve itself. However, the eventual end goal of a Python packaging ecosystem that integrates nicely with the various operating system software distribution mechanisms without excessive duplication of effort by developers and software packagers is a worthy one.<br><br><h2>Enhanced support for various concurrency mechanisms</h2>The addition of the 'concurrent' package, and its sole member, 'concurrent.futures' to the Python 3.2 standard library can be seen as a statement of intent to provide improved support for a variety of concurrency models within the standard library.<br><br><a href="http://www.python.org/dev/peps/pep-3153/">PEP 3153</a> ("Asynchronous IO support") is an early draft of an API standardisation PEP that will hopefully provide a basis for interoperability between components of the various Python asynchronous frameworks. It's a little hard to follow for non-Twisted developers at this stage, since there's no specific proposed stdlib API, nor any examples showing how it would allow things like plugging a Twisted protocol or transport into the gevent event loop, or vice-versa.<br><br>While developers are still ultimately responsible for understanding the interactions between threading and multiprocessing on their platform when using those modules, there are things CPython could be doing to make it easier on them. Jesse Noller, the current maintainer of the multiprocessing package, is looking for someone to take over the responsibility for coordinating the related development efforts (between cross-platform issues and the vagaries of os.fork interactions with threading, this is a non-trivial task, even with the expertise already available amongst the existing core developers).<br><br><a href="http://www.python.org/dev/peps/pep-3143/">PEP 3143</a> ("Standard daemon process library") may be given another look for 3.3, although it isn't clear if sufficient benefit would be gained from having that module in the stdlib rather than published via PyPI as it is currently.<br><br><h2>Providing a better native Python experience on Windows</h2>Windows is designed and built on the basis of a mindset that expects most end users to be good little consumers that buy software from developers rather than writing it themselves. Anyone writing software is thus assumed to be doing so for a living, and hence paying for professional development tools. Getting a decent development environment set up for free can be so tedious and painful that many developers (like me!) would rather just install and use Linux for development on our own time, even if we're happy enough to use Windows as a consumer (again, like me - mostly for games in my case). The non-existent "BUNDLE ALL THE THINGS!" approach to dependency management and the lack of any kind of consistent software updating interface just makes this worse, as does the fact that any platform API handling almost always has to special-case Windows, since their libc implementation is one of the more practically useless things on the face of this Earth.<br><br>This creates a vicious cycle in open source, where most of the active developers are using either Linux or Mac OS X, so their tools and development processes are focused on those platforms. They're also typically very command line oriented, since the diverse range of graphical environments and the widespread usage of remote terminal sessions makes command line utilities the most reliable and portable. Quite a few new developers coming into the fold run into the Windows roadblocks and either give up or switch to Linux for development (e.g. by grabbing VirtualBox and running a preconfigured Linux VM to avoid any hardware driver issues), thus perpetuating the cycle.<br><br>Personally, I blame the cross-platform hobbyist developer hostile nature of the platform on the Windows ecosystem itself and generally refuse to write code on it unless someone's paying me to do so. Fortunately for single-platform Windows users and developers, though, there are other open source devs that don't feel the same way. Some of them are working on tools (such as the script launcher described in <a href="http://www.python.org/dev/peps/pep-0397/">PEP 397</a>) that will make it easier to get usable development environments set up for new Python users.<br><br><h2>Miscellaneous API improvements in the standard library</h2>There are some areas of the standard library that are notoriously clumsy to use. HTTP/HTTPS comes to mind, as does date/time manipulation and filesystem access.<br><br>It's hard to find modules that strike the sweet spot of being well tested, well documented and proven in real world usage, with mature, stable APIs that aren't overly baroque and complicated (yes, yes, I realise the irony of that criticism given the APIs I'm talking about replacing or supplementing). It's even harder to find such modules in the hands of maintainers that are willing to consider supporting them as part of Python's standard library (potentially with backports to older versions of Python) rather than as standalone modules.<br><br><a href="http://www.python.org/dev/peps/pep-3151/">PEP 3151</a> ("Reworking the OS and IO exception hierarchy") is an effort to make filesystem and OS related exceptions easier to generate and handle correctly that is almost certain to appear in Python 3.3 in one form or another.<br><br>In the HTTP/HTTPS space, I have some hopes regarding Kenneth Reitz's 'requests' module, but it's going to take more real world experience with it and stabilisation of the API before that idea can be given any serious consideration.<br><br>The regex module (available on PyPI) is also often kicked around as a possible candidate for addition. Given the benefits it offers over and above the existing re module, it wouldn't surprise me to see a genuine push for its inclusion by default in 3.3 or some later version.<br><br>A basic stats library in the stdlib (to help keep people from implementing their own variants, usually badly) is also a possibility, but again, depends on a suitable candidate being identified and put forward by people willing to maintain it.<br><br><h2>The RFC treadmill</h2>Many parts of the Python standard library implement various RFCs and other standards. While our release cycle isn't fast enough to track the newer, still evolving protocols, it's good to provide native support for the latest versions of mature, widely supported ones.<br><br>For example, support for sendmg(), recvmsg() and recvmsg_into() methods will be available on socket objects in 3.3 (at least on POSIX-compatible platforms). Additions like <a href="http://www.python.org/dev/peps/pep-3144/">PEP 3144</a> ("IP Address Manipulation Library for the Python Standard Library") are also a potential possibility (although that particular example currently appears to be lacking a champion).<br><br><h2>Defining Python-specific interoperability standards</h2>Courtesy of the PEP process, python-dev often plays a role in defining and documenting interfaces that allow various Python frameworks to play nicely together, even if third party libraries are needed to make full use of them. WSGI, the Web Server Gateway Interface, is probably the most well-known example of this (the current version of that standard is documented in <a href="http://www.python.org/dev/peps/pep-3333/">PEP 3333</a>). Other examples include the database API, cryptographic algorithm APIs, and, of course, the assorted standards relating to packaging and distribution.<br><br>PEP 3333 was a minimalist update to WSGI to make it Python 3 compatible, so, as of 3.2, it's feasible for web frameworks to start considering Python 3 releases (previously, such releases would have been rather challenging, since it was unclear how they should talk to the underlying web server). It likely isn't the final word on the topic though, as the web-sig folks still kick around ideas like <a href="http://www.python.org/dev/peps/pep-0444/">PEP 444</a> and <a href="http://dirtsimple.org/2011/07/wsgi-is-dead-long-live-wsgi-lite.html">WSGI Lite</a> <a href="http://dirtsimple.org/2011/08/is-wsgi-lite-library-or-protocol-and.html">WSGI Lite</a>. Whether anything actually happens on that front or if we keep chugging along with an attitude of "eh, WSGI gets the job done, quit messing with it" is an open question (and far from my area of expertise).<br><br>One area that is being actively worked on, and will hopefully improve significantly in Python 3.3, is the low level buffer interface protocol defined by <a href="http://www.python.org/dev/peps/pep-3118/">PEP 3118</a>. It turned out that there were a few flaws in the way that PEP was implemented in CPython, so even though it did achieve the basic goal of allowing projects like NumPy and PIL to interoperate without needing to copy large data buffers around, there are still some rather rough edges in the way the protocol is exposed to Python code via memoryview objects, as well as in the respective obligations of code that produces and consumes these buffers. More details can be found on the <a href="http://bugs.python.org/issue10181">issue tracker</a> if you're interested in the gory details of defining conventions for reliable management of shared dynamically allocated memory in C and C++ code :)<br><br><h2>Enhancing and refining the import system</h2>The importer protocol defined by <a href="http://www.python.org/dev/peps/pep-0302/">PEP 302</a> was an ambitious attempt to decouple Python's import semantics from the underlying filesystem. It never fully succeeded - there's still a lot of Python code (including one or two components in the standard library!) that assumes the classical directories-on-a-disk layout for Python packages. That package layout, with the requirement for __init__.py files and restriction to a single directory for each package, is itself surprising and unintuitive for many Python novices.<br><br>There are a few suggestions in the works ultimately aimed not only at cleaning up how all this is implemented, but also at further decoupling the module hierarchy from the on-disk file layout. There are significant challenges in doing this in a way that makes life easier for developers writing new packages, while also allowing those writing tools that manipulate packages to more easily do the right thing, but it's an area that can definitely do with some rationalisation.<br><br><h2>Improved tools for factoring out and reusing code</h2>Python 3.3 will at least bring with it <a href="http://www.python.org/dev/peps/pep-0380/">PEP 380's</a> "yield from" expression which makes it easy to take part of a generator and split it out into a subgenerator without affecting the overall semantics of the original generator (doing this correctly in the general case is currently significantly harder than you might think - see the PEP for details).<br><br>I suspect that the next few years may bring some more tweaks to generators, generator expressions and context managers to make for loops and with statements even more powerful utilities for factoring out code. However, any adjustments in this area will be carefully balanced against the need for language stability and keeping things reasonably easy to learn.<br><br><h1>Not Even on the Radar</h1>In addition to the various suggestions mentioned in PEP 3099 ("Things that Will Not Change in Python 3000"), there are a couple of specific items I think are worth calling out as explicitly <i>not</i> being part of any realistic road map:<br><br><h2>Major internal architectural changes for CPython</h2>Questions like 'Why not remove the GIL?', 'Why not switch to a register based VM?', 'Why not add a JIT to CPython?' and 'Why not make PyPy the reference interpreter?' don't really have straightforward obvious answers other than "that's harder than you might think and probably less beneficial than you might hope", so I expect people to continue asking them indefinitely. However, I also don't see any significant changes coming on any of these fronts any time soon.<br><br>One of the key advantages of the CPython interpreter as a reference implementation is that it is, fundamentally, quite a <i>simple</i> beast (despite a few highly sophisticated corners). If we can do things, chances are that the other implementations are also going to be able to support them. By contrast, the translation stage in their toolchain means that PyPy can contemplate features like the existing JIT, or their current exploration of Software Transactional Memory for removing their Global Interpreter Lock in a useful way that simply aren't feasible with less sophisticated tools (or a much bigger development team).<br><br>Personally, I think the status quo in this space is in a pretty good place, with python-dev and CPython handling the evolution of the language specification itself, as well as providing an implementation that will work reasonably well on almost any platform with a C compiler (and preferably some level of POSIX compliance), while the PyPy crew focus on providing a fast, customisable implementation for the major platforms without getting distracted by arguments about possible new language features.<br><br><h2>More feature backports to the 2.x series</h2>Aside from the serious backwards compatibility problems accompanying the Unicode transition, the Py3k transition was also strongly influenced by the concept of paying down technical debt. Having legacy cruft lying around made it harder to introduce language <i>improvements</i>, since the additional interactions created more complexity to deal with, to the point where people just didn't want to bother any more.<br><br>By clearing out a bunch of that legacy baggage, we created a better platform for <i>new</i> improvements, like more pervasive (and efficient!) Unicode support, even richer metaclass capabilities, exception chaining, more intuitive division semantics and so on.<br><br>The cost, borne mostly by existing Python developers, is a long, slow transition over several years as people deal with the process of not only checking whether the automated tools can correctly handle their own code, but also waiting for all of their dependencies to be available on the updated platform. This actually seems to be going fairly well so far, even though people can be quite vocal in expressing their impatience with the current rate of progress.<br><br>Now, all of the major Python implementations are open source, so it's certainly <i>possible</i> for motivated developers to fork one of those implementations and start backporting features of interest from 3.x that aren't already available as downloads from PyPI (e.g. it's easy to download unittest2 to get access to 3.x unittest enhancements, so there's no reason to fork just for that kind of backport). However, anyone doing so will be taking up the task in full knowledge of the fact that the <i>existing</i> CPython development team found that process annoying and tedious enough that we got tired of doing it after two releases (2.6 and 2.7).<br><br>So, while I definitely expect a certain level of ongoing griping about this point, but I don't expect it to rise to the level of anyone doing the work to backport things like function annotations or exception chaining to the 2.x series.<br><i><br>Update 1: Added missing section about RFC support<br>Update 2: Fix sentence about role of technical debt in Py3k transition<br>Update 3: Added a link to a relevant post from Guido regarding the challenges of maintaining official road maps vs one-off "where are we now?" snapshots in time (like this post)<br>Update 4: Added missing section on defining Python-specific standards<br></i>
            
    <p>
        <a href="posts/201108of-python-and-road-maps-or-lack-thereof.html#disqus_thread" data-disqus-identifier="cache/posts/201108of-python-and-road-maps-or-lack-thereof.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201107sharing-vs-broadcasting.html">Sharing vs broadcasting</a>
        <small>  
             Posted: <time class="published" datetime="2011-07-16T17:04:00">2011-07-16 17:04</time>
        </small></h1>
        <hr>
        <p>Even since I started playing with Google+ and its Circle mechanic, I've been trying to figure out why I don't like it. I thought it sounded great when I heard about it, but as soon as I started using it... meh :P<br><br>I still haven't quite figured it out (although I mostly think it's the neither-chicken-nor-fowl aspect, along with it adding back ~500 useless "suggestions" from random Python mailing list contacts that I had already purged once), but it's certainly helped me see the way I communicate online in a different light.<br><br>For a long time, my only real online presence was regular postings to Python mailing lists, commenting on various sites and the very occasional post here on B&amp;L.<br><br>After enough of my friends joined, Facebook was added to the mix, but I do everything I can to keep that locked down so only acquaintances can see most things I post there.<br><br>After going to PyconAU last year, and in the leadup to PyCon US earlier this year, I started up my @ncoghlan_dev account on Twitter, got the "python" category tag on here added to Planet Python, and started to develop a bit more of an online public presence.<br><br>Here on the blog, I can, and do, tag each post according to areas of interest. As far as I know, the 'python' tag is the only one with any significant readership (due to the aggregation via Planet Python), but people could subscribe to the philosophy or metablogging tags if they really wanted to.<br><br>When it comes to sharing information about myself, there's really only a few levels based on how much I trust the people involved: Friends&amp;Family, Acquaintances, General Public pretty much covers it. Currently I handle that via a locked down FB for Friends, Family &amp; Acquaintances (with a "Limited Access" list for people that I exclude from some things, like tagged photos) and completely public material on Twitter and Blogger.<br><br>The public stuff is mostly Python related, since that's my main online presence, but I'm fairly open about political and philosophical matters as well. FB, by contrast, rarely sees any mention of Python at all (and I'm often a little more restrained on the political and philosophical front).<br><br>Where I think Circles goes wrong is that it conflates <i>Access Control</i> with <i>Topic Tagging</i>. When I publish Python stuff, I'm quite happy for it to be public. However, I'd also be happy to tag it with "python", just as I do here on the blog, to make it easier for my friends to decide which of my updates they want to see.<br><br>This is classic Publish/Subscribe architecture thinking. When Publishing, I normally want to be able to decide who *can* access stuff. That is limited by closeness, but typically unrelated to the topic. Having, tagging as a service to my subscribers, I am quite happy to do. When Subscribing, I want to be able to filter by topic.<br><br>If I publish something to, say, my Pythonistas circle, than that does more than I want. Yes, it publishes it to them, but it also locks out everybody else. The ways I know people and the degree to which I trust them do not align well with the topics I like to talk about. I've already seen quite a few cases where the reshared versions of public statuses I have received have been limited access.<br><br>The more I think about it, the more I think I'm going to stick with my current policy of using it as a public micro-blog and pretty much ignore the fact that the Circles page exists.</p>
            
    <p>
        <a href="posts/201107sharing-vs-broadcasting.html#disqus_thread" data-disqus-identifier="cache/posts/201107sharing-vs-broadcasting.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201107effective-communication-brain-hacking.html">Effective communication, brain hacking and diversity</a>
        <small>  
             Posted: <time class="published" datetime="2011-07-10T07:10:00">2011-07-10 07:10</time>
        </small></h1>
        <hr>
        <a href="https://twitter.com/#!/jessenoller">Jesse Noller</a> recently posted an interesting quote on Twitter:<br><blockquote>"One who feels hurt while listening to harsh language may lose his mindfulness and not hear what the other person is really saying."</blockquote>When you think about it, human language is a truly awe inspiring tool. By the simple act of creating certain vibrations in the air, marks on a page or electromagnetic patterns in a storage system, we're able to project our thoughts and feelings across space and time, using them to shape the thoughts and feelings of others.<br><br>While this ability to communicate is so thoroughly natural to most of us as humans that we typically take it for granted, it is actually an amazing world shaping capability <a href="http://www.youtube.com/watch?v=J7E-aoXLZGY">deserving of our respect and attention</a>.<br><br>And once we start giving it the attention it deserves, then we realise that we can judge the <i>effectiveness</i> of our own communication by looking at the communication that is subsequently reflected back at us. How well do those reflections mirror the thoughts and feelings that our own words were intended to create? It's the linguistic equivalent of running our code and seeing if it does what we wanted it to do.<br><br>All communication is a form of brain hacking, even when the only target is ourselves. We write lists of goals - formulating for our own benefit concrete plans of action that we can then tackle one step at a time. We write polemics, trying to engender in others some dim sense of the joy or outrage we feel with respect to certain topics. Sometimes we succeed, sometimes we fail. Sometimes we assume certain shared beliefs and understanding, so the point completely fails to come across to those without that common background.<br><br>For those closest to us, those with the most shared history, we have a rich tapestry of common knowledge to draw from. Movies we've all seen, books we've all read, events we all attended, discussions we were all part of - outsiders attempting to follow a transcript of our conversations would likely soon be utterly lost due to the shared subtext that isn't explicitly articulated (and the same is true for any group of close friends).<br><br>As groups get larger, the amount of truly common knowledge decreases, but there's still plenty of unwritten subtext that backs up whatever is explicitly articulated. In a certain sense, that unwritten subtext can be seen as the very <i>definition</i> of culture - it's the things you don't have to say because they're assumed. My past <a href="http://www.boredomandlaziness.org/2011/04/musings-on-culture-of-python-dev.html">musings on the culture of python-dev</a> are an example of this.<br><br>And that brings us to the point of considering the opening quote, diversity and questions of common courtesy. When speaking to friends, I can share truly awful jokes without offence because of the shared background information as to what is and isn't acceptable (and what things should be taken seriously). As the group being addressed gets larger, then the valid assumptions I can make about shared views of the world become fewer and fewer, so I have to start explicitly articulating things I would otherwise assume, and simply not say some things because I know (or at least strongly suspect) that they won't come across correctly to the audience I'm attempting to reach. Sometimes even addressing similar groups of people in a different context can change the assumptions as to what is a reasonable way to phrase things.<br><br>If a member of my target audience gets hung up on my wording or my choice of examples to the point where they miss the underlying message, then to a large degree, the responsibility lies with <i>me</i> as the originator of the communication. Now, I'm not a saint and make no pretence of being one. The rich fields of metaphors in English include many relating to subjects that are truly quite horrific or otherwise offensive to various groups of people. Sometimes I'm going to use that kind of phrasing without thinking about it, especially when talking rather than writing (my own <a href="http://www.mit.edu/~jcb/tact.html">innate tact filter</a> is definitely set up to filter incoming communication, so applying tact in the outwards direction is a conscious process rather than something I do automatically). If such a miscommunication happens and someone points it out, then the onus is on me to admit that yes, my choice of words was poor and obscured my meaning rather than illuminating it. That's life, I make mistakes, and hopefully we can move on.<br><br>It's not entirely a one way street, though. Just as we apply contextual analysis to our understanding of historical writings, so it can be useful to apply the same approach to things that are said by <i>current</i> figures. Richard Dawkins recently made some ill-advised comments in relation to <a href="http://skepchick.org">Skepchick's</a> advice to men to avoid certain actions that make them look creepy (that's all she said, "Don't do this, it's creepy" and she copped flack for it, as if she'd said people doing it should be sent to prison or castrated or something equally extreme). Does the fact that Dawkins clearly didn't get why he was <a href="http://skepchick.org/2011/07/dear-richard-dawkins/">in the wrong</a> make him a horrible human being or devalue his extensive contributions to our understanding of evolutionary biology*? No, it doesn't, any more than Isaac Newton's obsession with alchemy devalued his contributions to physics and mathematics. It just makes him a product of the culture that raised him. Hopefully he'll eventually realise this and publicly apologise for failing to give the matter due consideration before weighing in.<br><br>However, what really surprised me is the number of people that indicate they're <i>shocked</i> by his words, or questioning their support for his other activities just because he so vividly demonstrated his cluelessness on this particular topic. The world is a complicated place and the social dynamics of privilege, cultural blindspots and effectively encouraging diversity aren't one of the easiest pieces to comprehend. Hell, as a middle-class, 30-something, white, English-speaking, straight, cisgendered male living in Australia I'm quite certain that my own grasp of the topic is heavily coloured by the fact that on pretty much <i>any</i> of the typical grounds for discrimination I'm in the favoured majority (being an atheist is arguably the only exception, but that's far less of a problem here in Australia than it is in the US. Our Prime Minister is an acknowledged atheist and even the Murdoch media machine didn't really try to make much of an issue out of that before the last election). I do my best to understand the topic of diversity based on the experiences of those that actually have to deal with it on a daily basis, but it's still a far cry from seeing things first hand.<br><br>So, since I don't believe I can speak credibly to the topic of diversity directly, I instead prefer to encourage people to reflect on the value and nature of communication and community in general. Martin Fowler <a href="http://martinfowler.com/bliki/SmutOnRails.html">wrote an excellent piece</a> about the challenge of creating communities that are welcoming to a diverse audience without rendering them bland and humourless (as the <a href="https://secure.wikimedia.org/wikipedia/en/wiki/Theories_of_humor#Benign_Violation_Theory">benign violations</a> of expectations and assumptions that are at the heart of most humour often depend on the shared context that welcoming communities can't necessarily assume). This <a href="http://www.youtube.com/watch?v=b0Ti-gkJiXc">excellent video</a> highlights the importance of focusing on <i>actions</i> (e.g. "This thing you said was inappropriate and you should consider apologising for saying it") rather than <i>attributes</i> (e.g. "You are a racist/misogynist/whatever"). If the latter is actually true, you're unlikely to change their mind and if it *isn't* true, you're likely to miss an opportunity to educate them as they get defensive and stop listening (refer back to that opening quote!).<br><br>I don't pretend to have all (or even any of) the answers, I just believe the entire topic of effective communication and all it entails is one worthy of our collective consideration, since effective communication is almost always a necessary precursor to taking effective <i>action</i> (e.g. on matters such as <a href="http://www.boredomandlaziness.org/2011/03/climate-change-skepticism-text-book.html">mitigating and coping with climate change</a>).<br><br>In many respects though, the entire topic is really quite simple. To quote Abe Lincoln in one of my all time favourite movies:<blockquote>Be excellent to each other.</blockquote><br>----<br><i>* Seriously, read the popular science books on biology that Dawkins has written, especially "The Greatest Show on Earth". They're orders of magnitudes better than "The God Delusion", which is far too laden with angry and aggressive undertones to be an effective tool for communicating with anyone that doesn't already agree with the thesis of the book. In his biology books, his obvious love and passion for the subject matter comes to the fore and they're by far the better for it.<br><br><b>Note for Planet Python:</b> even though this post is about communication rather than code, I have included the python tag since a couple of different diversity related issues have come up recently on python-dev and psf-members. It is no coincidence that "communication" and "community" share a common(!) root in "communis".</i>
            
    <p>
        <a href="posts/201107effective-communication-brain-hacking.html#disqus_thread" data-disqus-identifier="cache/posts/201107effective-communication-brain-hacking.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201107sure-its-surprising-but-whats.html">Sure it's surprising, but what's the alternative?</a>
        <small>  
             Posted: <time class="published" datetime="2011-07-09T12:12:00">2011-07-09 12:12</time>
        </small></h1>
        <hr>
        <p>Armin Ronacher (aka @mitsuhiko) did a really nice job of explaining <a href="http://lucumr.pocoo.org/2011/7/9/python-and-pola/">some of the behaviours of Python</a> that are often confusing to developers coming from other languages.<br><br>However, in that article, he also commented on some of the behaviours of Python that <i>he</i> still considers surprising and questions whether or not he would retain them in the hypothetical event of designing a new Python-inspired language that had the chance to do something different.<br><br>My perspective on two of the behaviours he listed is that they're items that are affected by fundamental underlying concepts that we really don't explain well even to current Python users. This reply is intended to be a small step towards correcting that. I mostly agree with him on the third, but I really don't know what could be done as an alternative.<br><br></p><h3>The dual life of "."</h3><br>Addressing the case where I mostly agree with Armin first, the period has two main uses in Python: as part of floating point and complex number literals and as the identifier separator for attribute access. These two uses collide when it comes to accessing attributes directly on integer literals. Historically that wasn't an issue, since integers didn't really have any interesting attributes anyway.<br><br>However, with the addition of the "bit_length" method and the introduction of the standardised numeric tower (and the associated methods and attributes inherited from the Complex and Rational ABCs), integers now have some public attributes in addition to the special method implementations they have always provided. That means we'll sometimes see things like:<br><pre class="brush: py">1000000 .bit_length()</pre>to avoid this confusing error:<br><pre class="brush: py">&gt;&gt;&gt; 1000000.bit_length()<br>  File "stdin", line 1<br>    1000000.bit_length()<br>                     ^<br>SyntaxError: invalid syntax<br></pre>This could definitely be avoided by abandoning Guido's rule that parsing Python shall not require anything more sophisticated than an <a href="https://secure.wikimedia.org/wikipedia/en/wiki/LL_parser">LL(1) parser</a> and require the parser to backtrack when float parsing fails and reinterpret the operation as attribute access instead. (That said, looking at the token stream, I'm now wondering if it may even be possible to fix this within the constraints of LL(1) - the tokenizer emits two tokens for "1.bit_length", but only one for something like "1.e16". I'm not sure the concept can be expressed in the Grammar in a way that the CPython parser generator would understand, though)<br><br><h3>Decorators vs decorator factories</h3><br>This is the simpler case of the two where I think we have a documentation and education problem rather than a fundamental design flaw, and it stems largely from a bit of careless terminology: the word "decorator" is used widely to refer not only to actual decorators but also to decorator <i>factories</i>. Decorator expressions (the bit after a "@" on a line preceding a function header) are required to produce callables that accept a single argument (typically the function being defined) and return a result that will be bound to the name used in the function header line (usually either the original function or else some kind of wrapper around it). These expressions typically take one of two forms: they either reference an actual decorator by name, or else they will call a decorator factory to create an appropriate decorator at function definition time.<br><br>And that's where the sloppy terminology catches up with us: because we've loosely used the term "decorator" for both actual decorators and decorator factories since the early days of PEP 318, decorator implementators get surprised at how difficult the transition can be from "simple decorator" to "decorator with arguments". In reality, it is just as hard as <i>any</i> transition from providing a single instance of an object to instead providing a factory function that creates such instances, but the loose terminology obscures that.<br><br>I actually find this case to be somewhat analogous to the case of first class functions. Many developers coming to Python from languages with implicit call semantics (i.e. parentheses optional) get frustrated by the fact that Python demands they always supply the (to them) redundant parentheses. Of course, experienced Python programmers know that, due to the first class nature of functions in Python, "f" just refers to the function itself and "f()" is needed to actually call it.<br><br>The situation with decorator factories is similar. @classmethod is an actual decorator, so no parentheses are needed and we can just refer to it directly. Something like @functools.wraps on the other hand, is a decorator factory, so we need to call it if we want it to create a real decorator for us to use.<br><br><h3>Evaluating default parameters at function definition time</h3><br>This is another case where I think we have an underlying education and documentation problem and the confusion over mutable default arguments is just a symptom of that. To make this one extra special, it lies at the intersection of <i>two</i> basic points of confusion, only one of which is well publicised.<br><br>The immutable vs mutable confusion is well documented (and, indeed, Armin pointed it out in his article in the context of handling of ordinary function arguments) so I'm not going to repeat it here. The second, <a href="http://bugs.python.org/issue12374">less well documented</a> point of confusion is the lack of a clear explanation in the official documentation of the differences between compilation time (syntax checks), function definition time (decorators, default argument evaluation) and function execution time (argument binding, execution of function body). (Generators actually split that last part up even further)<br><br>However, Armin clearly understands both of those distinctions, so I can't chalk the objection in his particular case up to that explanation. Instead, I'm going to consider the question of "Well, what if Python didn't work that way?".<br><br>If default arguments aren't evaluated at definition time in the scope defining the function, what are the alternatives? The only alternative that readily presents itself is to keep the code objects around as distinct closures. As a point of history, Python had default arguments long before it had closures, so that provides a very practical reason why deferred evaluation of default argument expressions really wasn't an option. However, this is a hypothetical discussion, so we'll continue.<br><br>Now we get to the first serious objection: the performance hit. Instead of just moving a few object references around, in the absence of some fancy optimisation in the compiler, deferred evaluation of even basic default arguments like "x=1, y=2" is going to require multiple function calls to actually run the code in the closures. That may be feasible if you've got a sophisticated toolchain like PyPy backing you up but is a serious concern for simpler toolchains. Evaluating some expressions and stashing the results on the function object? That's easy. Delaying the evaluation and redoing it every time it's needed? Probably not too hard (as long as closures are already available). Making the latter as fast as the former for the simple, common cases (like immutable constants)? Damn hard.<br><br>But, let's further suppose we've done the work to handle the cases that allow constant folding nicely and we still cache those on the function object so we're not getting a big speed hit. What happens to our name lookup semantics from default argument expressions when we have deferred evaluation? Why, we get <i>closure</i> semantics of course, and those are simple and natural and never confused anybody, right? (if you believe I actually mean that, I have an Opera House to sell you...)<br><br>While Python's default argument handling is the way it is at least partially due to history (i.e. the lack of closures when it was implemented meant that storing the expressions results on the function object was the only viable way to do it), the additional runtime overhead and complexity in the implementation and semantics involved in delaying the evaluation makes me suspect that going the runtime evaluation path wouldn't necessarily be the clear win that Armin suggests it would be.
            
    <p>
        <a href="posts/201107sure-its-surprising-but-whats.html#disqus_thread" data-disqus-identifier="cache/posts/201107sure-its-surprising-but-whats.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201106updated-to-do-list-for-python-33.html">Updated to do list for Python 3.3</a>
        <small>  
             Posted: <time class="published" datetime="2011-06-29T05:27:00">2011-06-29 05:27</time>
        </small></h1>
        <hr>
        <p>I wrote a <a href="http://www.boredomandlaziness.org/2011/01/some-goals-for-python-33.html">to-do list</a> for 3.3 back in January. It's already obsolete, so inspired by Brett's <a href="http://sayspy.blogspot.com/2011/06/my-personal-plans-for-python-33.html">recent update</a> I have a new list of my own:<br><br></p><h3>Finish PEP 394 (The "python" command on *nix platforms)</h3>This is the PEP clarifying that the current collective recommendation from python-dev to Linux distributions (et al) is to stick with Python 2.x as the system python for the moment. It also involves adding a python2 symlink to the next release of 2.7 so that we follow our own advice.<br><br><h3>PEP 3118 buffer API and memoryview fixes</h3>On <a href="http://bugs.python.org/issue10181">issue #10181</a> I've been doing the software architect thing in devising a way to address the flaws in the design of memoryview objects. Absent further contributions I may resort to coding it myself, otherwise I'll at least review whatever patches are put forward.<br><br><h3>PEP 380 (yield from expressions)</h3>Guido gave his approval to PEP 380 recently. This is almost ready to go in, but <a href="http://bugs.python.org/issue11682">some work</a> needs to be done on incorporating the tests neatly into the regression test suite.<br><br><h3>Import engine</h3>Import engine is a GSoC project I am mentoring that consolidates the scattered state for the import system into methods and attributes on a single object. While the "real" import system state will remain in the current locations for compatibility reasons, the import engine API will provide a more coherent interface to the whole mechanism. It will also allow creation of more limited engine instances for targeted imports (e.g. from plugin directories) that won't be confused by name conflicts with other libraries.<br><br>I'm hopeful Greg will get as far as writing the PEP itself before the end of the summer, otherwise that will become a follow-up activity.<br><br><h3>CPython compiler enhancements</h3>Eugene Toder put together a <a href="http://bugs.python.org/issue11549">nice patch</a> that refactors the CPython compiler to include an AST optimisation step as well as cleaning up several 2.x holdovers in the AST. Getting the AST compiler ready for Python 2.5 is one of the first things I ever hacked on as a core developer and it's still one of my main interest areas.<br><br><h3>PEP 395 (module aliasing)</h3>After starting to work with Django and discovering it commits the sin of putting a package subdirectory on sys.path, I have even more motivation to provide a module aliasing mechanism that doesn't run the risk of accidentally getting multiple copies of the same module loaded in a process. That kind of thing should only happen when you do it on purpose.<br><br>I need to review my proposal in PEP 395 to make sure it also covers the Django use case (and modify the proposal if it doesn't).<br><br><h3>Core mentoring</h3>There are still some coverage patches from the Pycon US sprints to be reviewed and committed, along with a couple from the core mentoring mailing list. I'm also likely to be leading the CPython sprints following PyconAU, so there'll be some more patches to review and commit as a result of that.<br><br><h3>importlib bootstrapping</h3>While the migration to importlib is definitely Brett's project, it's also one I keep a close eye on in case he wants any reviews or feedback on elements of the design.<br><br><h3>PEP process tinkering</h3>I'd still like to clean up PEP 0 by adding the "Consensus" state to PEP 1. However, it has dropped below the above items on the to-do list.<br><br><h3>With statement tinkering</h3>Screwing up @contextmanager for 3.2 gives me even more motivation to want implicit context managers and Alex Gaynor tells me he has a use case for a context manager that can skip the with statement body. As with the PEP process tinkering, these are still on the "yes, please" list, but I'm unlikely to get a chance to work on them any time soon.<br><br><h3>Other miscellaneous features</h3>Resurrecting the post-import hooks PEP would be nice and I'd also like to add some of the ABC registration viewing and monitoring features proposed on the issue tracker.<br><br>So, how long until 3.3 again? :)
            
    <p>
        <a href="posts/201106updated-to-do-list-for-python-33.html#disqus_thread" data-disqus-identifier="cache/posts/201106updated-to-do-list-for-python-33.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201106switching-to-android.html">Switching to Android</a>
        <small>  
             Posted: <time class="published" datetime="2011-06-15T14:33:00">2011-06-15 14:33</time>
        </small></h1>
        <hr>
        <p>So my new HTC Desire S arrived today which means it is time for an app downloading spree... (starting with just freebies for now while I figure out what I do and don't like, and delete some of the cruft that HTC/Telstra added)<br><br>Already grabbed:<br>Google Goggles<br>Google Sky<br>Google Reader<br>Dropbox<br>Firefox<br>KeepassDroid<br>OI File Manager<br>Shelves for Android (now actually *scanning* my book collection will be quite a project...)<br>Barcode Scanner<br>Compass<br><br>And a ton of standard apps from Google/HTC/Telstra for all the basics (Phone, SMS, Music, Mail/Gmail, FB, Twitter, Camera, Calendar, Clock, Weather, Calculator, Adobe PDF Reader, Maps/Navigation, LED Flashlight, etc).<br><br>Things I know I want but don't have yet:<br><br>Ebook reader (I used Stanza from Lexcycle on the iPhone, but they don't make an Android version)<br>Weight tracker (don't need anything fancy, just something that I can import old data into and will give me a time-weighted average)<br><br>I'm also open to suggestions for things I might want but just don't know it yet, and of course I'll have to track down a few idle time games.</p>
            
    <p>
        <a href="posts/201106switching-to-android.html#disqus_thread" data-disqus-identifier="cache/posts/201106switching-to-android.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201106fixing-grub2-update-issues-with-kubuntu.html">Fixing GRUB2 update issues with Kubuntu 11.04</a>
        <small>  
             Posted: <time class="published" datetime="2011-06-13T12:35:00">2011-06-13 12:35</time>
        </small></h1>
        <hr>
        <p>After doing a dist-upgrade to 11.04 a while back, my Kubuntu machine refused to boot.<br><br>I eventually tracked this down to GRUB2 os-prober feature freaking out and trying to boot off the partition that held only the "/usr" directory rather than the one with the root and "/boot" directories. (Why, I have no idea. The latter is the first partition and the one that holds the MBR, so os-prober is clearly going to some effort to find and enforce the wrong partition).<br><br>After searching on Google and with a bit of experimentation, I was able to fix it by booting off a LiveCD, adding the line "GRUB_DISABLE_OS_PROBER=true" to "/etc/default/grub" and then running "sudo update-grub".</p>
            
    <p>
        <a href="posts/201106fixing-grub2-update-issues-with-kubuntu.html#disqus_thread" data-disqus-identifier="cache/posts/201106fixing-grub2-update-issues-with-kubuntu.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201104musings-on-culture-of-python-dev.html">Musings on the culture of python-dev</a>
        <small>  
             Posted: <time class="published" datetime="2011-04-21T15:42:00">2011-04-21 15:42</time>
        </small></h1>
        <hr>
        <p>I mentioned at the end of my <a href="http://www.boredomandlaziness.org/2011/03/thoughts-and-impressions-following.html">PyCon summary post</a> that several people had told me that they find python-dev to be a hostile and unwelcoming environment, and that (after some reflection on the matter) I could actually see their point. We may be the model of civility compared to somewhere like the Linux Kernel Mailing List or (*shudder*) Blizzard's World of Warcraft forums, but mere civility is a far cry from being a consciously welcoming place.<br><br>I'll say up front that <a href="http://mail.python.org/mailman/listinfo/python-dev">python-dev</a> itself is unlikely to change any time soon. There are reasons it is the way it is, and I'll elaborate on some of them later in this post. In the meantime, <a href="http://mail.python.org/mailman/listinfo/python-ideas">python-ideas</a> is available as a venue where outlandish ideas won't be rejected quite as abruptly, and the <a href="http://mail.python.org/mailman/listinfo/core-mentorship">core-mentorship</a> list has been set up specifically to provide a gentler introduction to the ways and means of core development without having to jump right in at the deep end by posting to python-ideas or python-dev. Questions about development <i>with</i> Python will continue to be redirected in fairly short order to <a href="http://mail.python.org/mailman/listinfo/python-list">python-list</a> or <a href="http://mail.python.org/mailman/listinfo/python-tutor">python-tutor</a>.<br><br>And, of course, what follows is just my opinion. Ask another veteran python-dev poster what they think, and you'll likely get a different answer. Ask an actual newcomer to (or lurker on) python-dev, and they'll probably have a different answer, too.<br><br></p><h2>Python evolves too slowly!</h2><h2>You're changing the language too fast!</h2>If I had to choose just one explanation for the frequency of abrupt responses in python-dev, the tension between the above two statements would be it. Compared to application-level open source projects, Python actually evolves quite slowly. 18+ months between minor version increments? 10 <i>years</i> between major versions? That's crazy! Canonical releases an entire new OS version every 6 months!<br><br>On the flip side, however, for a programming language definition, Python evolves quite fast. There's no such thing as a minor release for C or C++ - the last versions of those were C99 and C++98 respectively. We should see C++11 published by the end of the year, and C1X is still in development. CPython's own PEP 7 still mandates the use of C89 compatible constructs for portability reasons, even though C89 is older than one of our <i>release managers</i>.<br><br>Java hasn't had a major feature update since 2006, and even C# is only running at a new version every 2-3 years (with the last formally standardised version being 2.0 back in 2006).<br><br>Python also has a history of being more aggressive with deprecations than are languages backed by large corporate sponsors. We only have limited volunteer resources, so rather than letting old code bitrot (or else take up maintenance time when it breaks), we'd prefer to rip it out in favour of an improved alternative. However, "more aggressive" is still pretty slow - some deprecated features stuck around for nearly 10 years (until Python 3 came out) and even a "fast" deprecation has historically taken at least 3 years (x.y contains feature, x.y+1 deprecates it, x.y+2 removes it). It's highly likely that that deprecation period will be extended out by a release for the 3.x series, pushing the minimum lifetime of a new feature that later proves to be a mistake out to nearly 5 years.<br><br>The task of updating the language and the standard library is a balancing act between those two forces - we want to make life easier for programmers adopting Python for new activities, while preserving backwards compatibility for existing applications. This is why Python 3 is such a big deal - most decisions are made with the emphasis on the needs of existing Python programmers, but the decision to create Python 3 was largely for the benefit of <i>future</i> Python programmers. That means that all current Python programmers are lumped with the task of actually managing a disruptive transition that wasn't really designed for their immediate benefit. Obviously, the collective opinion of python-dev is that it will be worth the pain in the long run, but those anticipated benefits don't make the migration any easier to deal with right now.<br><br>We get quite a few people coming into python-dev and betraying quite quickly that they don't have any respect for the time frames involved in language (rather than application) development. Telling someone "You're wrong, but explaining the real reasons why would require that I distil decades of experience down into a single email post and I don't feel like taking the time to do that right now, since even if I tried you would ignore me anyway" tends to be difficult to phrase politely.<br><br>A lot of the rest of this post is really just elaborations on the theme of <i>why</i> a programming language needs to evolve more slowly than most other pieces of software.<br><br><h2>Heart of the ecosystem, but far from the whole of it</h2>I've elaborated on the cost of change before, when discussing the design principle <a href="http://www.boredomandlaziness.org/2011/02/status-quo-wins-stalemate.html">"Status Quo Wins a Stalemate"</a>. The core point is that any significant change made by python-dev, even one that will ultimately be a positive one, imposes a high near-term cost as the implications of the change ripple out across the whole Python ecosystem. As noted above, newcomers can easily perceive this as high-and-mighty arrogance rather than the voice of experience.<br><br><h2>What do you mean by "cognitive burden"?</h2>Even without considering the near-term cost of changes, every addition to the language (and even the standard library) imposes a potential burden on anyone <i>learning</i> the language in the future. You can't just say, "Oh, I won't worry about learning that feature" if the code base you've been asked to maintain uses it, or if it is offered as an answer to a query posted on python-list or Stack Overflow or the like. The principle of "There Should Be One - and preferably only one - Obvious Way To Do It" is aimed squarely at reducing the cognitive load on people trying to learn the language. Quite clearly, the "only one" aspect is an ideal rather than a practical reality (two kinds of string formatting and three argument parsing libraries in the standard library all say "Hi!"), but in such cases we do try to indicate that the most recently added (and hopefully least quirky) approach is the preferred way to do it.<br><br>This idea is also encountered as the aphorism "Not every three line function needs to be a builtin". Again, new posters may not take kindly to being told that their idea simply doesn't cut it as a potential language addition.<br><br><h2>Gee, how dumb are you lot? Why don't you just...?</h2>Another favourite bugbear is posters that bounce into python-dev assuming that we're a collection of clueless idiots that can't see the obvious. Collectively, we <i>do</i> pay quite a bit of attention to what other language communities are doing, as well as having personal experience with what does and doesn't work in actual programming practice. There's a reason that "new" Python features are generally modelled on something that has been demonstrated to work elsewhere (e.g. list comprehensions et al inspired by Haskell, the with statement partially inspired by C++ RAII, the new string formatting syntax inspired by C# string formatting).<br><br>New posters that give the list a tiny bit of credit and do us the courtesy of at least asking "Has this been thought of or discussed before? Are there any problems with it that I haven't considered?" tend to get <i>significantly</i> more positive reactions than those that start with a tone closer to "Here is my awesome idea, and you are seriously dumb if you don't get it and decide to adopt it immediately!". Positive responses are even more likely if ideas are posted to the right list (i.e. python-ideas).<br><br><h2>You do remember you didn't have to pay a cent for this, right?</h2>A fortunately rare (but still annoying when it arises) source of negative reactions is the end user that comes in demanding to know why certain things aren't being done, when the answer is "Because nobody stepped up to either do it themselves, or to pay for someone else to do it". It's a pretty simple equation, really, and not demonstrating understanding of it suggests a complete disregard for the volunteer nature of so many of the contributions that have been made to Python over the years.<br><br><h2>In the end, we're still just people</h2>We like <a href="http://bikeshed.org/">painting bikesheds</a> (or, more to the point, we can't always help ourselves, even when we know better). We like to be right and "win" arguments (or sometimes simply take time to process and properly understand the point someone else is trying to make). Even the mailing list members that are paid to work with Python by our employers are typically still participating in python-dev and hacking on CPython in our spare time rather than as a job, so there's not a lot of tolerance for "noise" and "time wasting".<br><br>As I'm a firm believer in the phrase "Vigorous criticism is the only known antidote to error", there are limits to how much I would personally <i>want</i> the culture of python-dev to change. Moving "blue sky" dreaming to python-ideas, "how does the process work?" coaching to core-mentorship and VCS management issues to python-committers allow them to develop cultures more appropriate to those specific activities, allowing python-dev to really focus in on the "vigorous criticism" part of the story. Keeping that from crossing the line into excessive negativity and a total reluctance to change is an ongoing challenge, but hopefully an awareness of that danger and the occasional pause for reflection will be enough to keep things on the right track.
            
    <p>
        <a href="posts/201104musings-on-culture-of-python-dev.html#disqus_thread" data-disqus-identifier="cache/posts/201104musings-on-culture-of-python-dev.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201104benefits-and-limitations-of-pyc-only.html">The benefits (and limitations) of PYC-only Python distribution</a>
        <small>  
             Posted: <time class="published" datetime="2011-04-09T12:47:00">2011-04-09 12:47</time>
        </small></h1>
        <hr>
        <a href="http://programmers.stackexchange.com/questions/66616/ways-to-prevent-client-seeing-my-code">This Stack Overflow question</a> hit my feed reader recently, prompting the usual discussion about the effectiveness of PYC only distribution as a mechanism for obfuscating Python code.<br><br><h3>PYC Only Distribution</h3>In case it isn't completely obvious from the name, PYC only distribution is a matter of taking your code base, running "compileall" (or an equivalent utility) over it to generate the .pyc files, and then removing all of the original .py source files from the distributed version.<br><br>Plenty of Python programmers (especially the pure open source ones) consider this practice an absolute travesty and would be quite happy to see it disallowed entirely. Early drafts of PEP 3147 (PYC Repository Directories) in fact proposed exactly that - in the absence of the associated source file, a compiled PYC file would have been ignored.<br><br>However, such blatant backwards incompatibility aroused protests from several parties (including me), and support for PYC-only distribution was restored in later versions of the PEP (although "compileall" now requires a command line switch in order to generate the files in the correct location for PYC-only distribution).<br><br><h3>Use Cases</h3>As I see it, there are a couple of legitimate use cases for PYC-only distribution:<br><ul><li>Embedded firmware: If your code is going onto an embedded system where space is at a premium, there's no point including both your source code <i>and</i> the PYC files. Better to just include the compiled ones, as that is all you really need</li><li>Cutting down on support calls (or at least making the ones you do get more comprehensible): Engineers and scientists like to tinker. It's in their nature. When they know just enough Python to be a danger to themselves and others, you can get some truly bizarre tickets if they've been fiddling with things and failed to revert their changes correctly (or didn't revert them at all). Shipping only the PYC files can help make sure the temptation to fiddle never even arises</li></ul><br>Of the two, the former is by far the stronger use case. The latter is attempting a technical solution to a social problem and those rarely work out well in the long run. Still, however arguable its merits, I personally consider deterrence of casual modifications a valid use case for the feature.<br><br><h3>Drawbacks</h3>Stripping the source code out of the distribution does involve some pretty serious drawbacks. The main one is the fact that you no longer have the ability to fall back to re-compilation if the embedded magic cookie doesn't match the execution environment.<br><br>This restricts practical PYC-only distribution to comparatively constrained environments that can ensure a matching version of Python is available to execute the PYC files, such as:<br><ul><li>Embedded systems</li><li>Corporate SOEs (Standard Operating Environments)</li><li>Bundled interpreters targeting a specific platform</li></ul><br>Cross-platform compatibility of PYC files (especially for 32-bit vs 64-bit and ARM vs x86) is also significantly less robust than the cross-platform compatibility of Python source code.<br><br><h3>Limitations</h3>Going back to the SO question that most recently got me thinking about this topic, the big limitation to keep in mind is this: <i>shipping only PYC files will not reliably keep anyone from reading your code</i>. While comments do get thrown away by the compilation process, and docstrings can be stripped with the "-OO" option, Python will always know the <i>names</i> of all the variables at runtime, so that information will always be present in the compiled bytecode. Given both the code structure and the original variable names, most decent programmers are going to be able to understand what the code was doing, even if they don't have access to the comments and docstrings.<br><br>While there aren't any currently active open source projects that provide full decompilation of CPython bytecode, such projects have existed in the past and could easily exist again in the future. There are also companies which provide Python decompilation as a paid service (decompyle and depython are the two that I am personally aware of).<br><br><h3>Alternatives</h3>You can deter casual tinkering reasonably well by placing your code in a zip archive with a non-standard extension (even .py!). If you prepend an appropriate shebang line, you can even mark it as executable on POSIX based systems (see <a href="http://www.boredomandlaziness.org/2011/03/what-is-python-script.html">this post</a> for more information).<br><br>You could also write your code in Cython or RPython instead of vanilla Python and ship fully compiled executable binaries.<br><br>There are minifier projects for Python (such as <a href="http://pypi.python.org/pypi/mnfy">mnfy</a>) that could be fairly readily adapted to perform obfuscation tricks (such as replacing meaningful variable names with uninformative terms like "_id1").
            
    <p>
        <a href="posts/201104benefits-and-limitations-of-pyc-only.html#disqus_thread" data-disqus-identifier="cache/posts/201104benefits-and-limitations-of-pyc-only.html">Comments</a>

        </p></div>
        <div class="postbox">
        <h1><a href="posts/201103climate-change-skepticism-text-book.html">Climate change "skepticism": a text book case of the "Tragedy of the Commons"</a>
        <small>  
             Posted: <time class="published" datetime="2011-03-29T14:44:00">2011-03-29 14:44</time>
        </small></h1>
        <hr>
        <p>So, with the government moving to implement a carbon tax, the issue of climate change and attempts to mitigate it are once again a big deal here in Australia.<br><br>Inspired by a friend's comment comparing carbon taxes with fines for littering, along with a few of the  responses to that comment, I started pondering the similarities and differences between the current attempts to deal with carbon dioxide emissions, and the <a href="http://www.edf.org/page.cfm?tagID=1085">effective cost internalisation schemes created</a> in the 1990's that severely reduced industrial sulphur dioxide emissions, as well as various current laws that prohibit dumping of toxic waste in developed nations (and their <a href="http://www.ehow.com/info_8012059_causes-dumping-third-world-countries.html">unintended side effects</a>).<br><br><i>(Fair warning before I wade in: I've simplified quite a few things, particularly on the acid rain front. This post is long enough as it is, without making it even longer with all the relevant caveats about other causes of acid rain and any other details I've glossed over as being irrelevant to the main point of the article. It's a general overview, not a peer reviewed scientific paper)</i><br><br><b>A tale of two gases</b><br>The key similarity between sulphur dioxide induced acid rain and carbon dioxide induced global warming is hopefully fairly obvious: they both represent a classic economic "externality", a cost borne by someone other than the person responsible for causing it. The industries and individuals emitting these pollutants don't suffer any immediate harmful consequence through the normal action of nature.<br><br>In situations like that, regulation in one form or another is the only effective tool we have to ensure that at least some portion of those external costs is reflected back on the responsible individuals. As noted in the link above, this approach proved extraordinarily effective in reducing acid rain in the US, drastically cutting sulphur emissions at a fraction of the originally predicted cost.<br><br>However, there are a few key differences that have contributed to the discussion over carbon dioxide taking a rather different path to that over sulphur dioxide:<br></p><ol><li>It's hard to deny the harmful effects of sulphur dioxide emissions when plants, fish and insects are dying due to the excessive levels of acidity in rain and ground water. By contrast, it is very hard to convey clearly why a small rise in average global temperatures is going to be such a bad thing, or even how human carbon dioxide emissions contribute to that happening.</li><li>Acid rain from sulphur emissions is generally a <i>local</i> problem with local causes (from a continental perspective, anyway). Stop emitting sulphur dioxide in the US and acid rain stops falling in the US. Significantly reduce carbon dioxide emissions in Australia (or even the US) though, and we're likely still screwed if other countries don't follow suit.</li><li>The "sulphur" cycle (how long it takes for the sulphur emissions to be deposited back on the ground as acid rain) is significantly shorter than that of carbon, so efforts to reduce emissions will have an effect on water acidity levels in a reasonably short time frame</li></ol>Comparisons with anti-waste dumping laws are similar: the effects of toxic waste dumping are generally both local, obvious and able to be cleaned up within years rather than decades, so it is easy to get support for laws prohibiting such practices, even if the end result of those laws is just to export the problem to a poorer country.<br><br><b>Manufacturing doubt and exploiting the "Grey Fallacy"</b><br>Acid rain is an easy problem to sell: the acid damages plants and objects directly, so the harm can be easily conveyed through pictures and videos. The problem of climate change, though is far, far harder to illustrate simply, since it is a signal buried in some very noisy weather patterns.<br><br>A fundamental issue is the fact that people don't really experience climate, but instead experience weather. The day to day and seasonal variations in the weather massively exceed the scale of the underlying trends being discussed by climate scientists. The fact that predicting the weather in a few days time is harder than predicting long term climate trends is actually massively counterintuitive, even though almost all scientists and engineers are familiar with the fact that random fluctuations at a small scale may average out to something almost perfectly predictable at a large scale (compare the unpredictable nature of quantum physics with the straightforward determinism of classical Newtonian mechanics).<br><br>We're also not really used to the idea of directly affecting the balance of the planet on a global scale. In absolute terms, the effects of the sun, the oceans and the plants on the carbon cycle all dwarf the human contribution. The idea that the non-human forcings were actually largely in balance with each other, and that the comparatively small human contribution is enough to tip that balance to the point of producing more carbon than can be absorbed by natural mechanisms is really quite a subtle one.<br><br>The scientific background needed to truly understand and come to grips with the hows and the whys of the IPCC predictions reminds me of <a href="http://abstrusegoose.com/272">this comic</a> regarding the knowledge needed to even begin to understand the vagaries of string theory (make sure to click through on the comic itself to see the whole chain of images).<br><br>If there wasn't anyone around with a vested interest in maintaining the status quo (at least for a while longer), this likely wouldn't be a problem. Scientists, politicians, economists and engineers could proceed with the development of mitigation strategies, while also attempting to educate the general populace as to the reasons behind any actions taken. Since such vested interests do exist, however, their opposition makes it significantly harder to convince the lay public that there is a real problem here.<br><br>Because climate science is such a complex topic, it is actually quite hard to explain to a non-scientist just <i>why</i> the scientific consensus is as strong as it is. If you oversimplify, you veer into territory where statements are so oversimplified that they border on being false. However, if you're coming from the other angle and want to persuade people that the science "isn't settled", then you're no longer constrained by the need to be accurate and can just go with whatever sounds more intuitively plausible and/or better caters to people's natural inclinations and prejudices.<br><br>Sites like <a href="http://www.skepticalscience.com/">Skeptical Science</a> do their best to clearly explain why the oft-repeated "skeptic" arguments are basically BS (and do it well), but to someone only following the Cliff Notes mainstream media version of the debate, the temptation is still very, very strong to just assume the truth lies somewhere between the two positions being debated.<br><br>Telling people to "do their own research" doesn't really help all that much in practice. Telling the BS from the valid science is itself a fine art that takes a great deal of skill and experience. Being a veteran of arguments with creationists (and even intelligent design advocates) is actually quite beneficial, since the global warming "skeptics" use many of the same rhetorical tricks as creationists do when attempting to deny the fact of evolution (incessant use of such deceptive tactics is actually one of the major hints that someone is trying to sell you a line rather than just stating the truth as they see it). The most common tactic used by both groups is to drag out a thoroughly rebutted argument for each new audience, in hopes that they aren't aware of the existence of a rebuttal.<br><br>For example, just as most people can't answer "How could a bombardier beetle possibly evolve?" off the top of their heads - I've actually forgotten all of the plausible answers myself - neither can they answer questions like "If the climate is supposed to be warming, why is it colder now than it was during the Middle Ages?". While that one is actually fairly straightforward to answer (ice cores and other data shows that the medieval warming was likely localised to Europe due to various currents in the Atlantic, but this merely shifted heat around, so that other parts of the world were correspondingly colder), there are dozens of other oft-repeated thoroughly debunked arguments, and it's basically impossible for a mere interested observer to remember them all. As it turns out, I had actually misremembered the correct explanation for the Medieval warm period, so the point above isn't quite right and invites an attack on the grounds that I don't know what I'm what I'm talking about. To some extent that's actually true - my opinion on climate science issues is based on a meta-analysis of the trustworthiness of various information sources (including the full <a href="http://www.ipcc.ch/publications_and_data/publications_and_data_reports.shtml">IPCC reports</a>), since I'm not inclined to spend a few decades studying up and redoing all the science myself. Fortunately, Skeptical Science has the <a href="http://www.skepticalscience.com/medieval-warm-period.htm">full story</a> for this question and many others (the MWP was actually colder than the latter half of this century, despite higher solar activity and lower volcanic activity), so correcting my error is the task of a few moments. And if you're still wondering about the bombardier beetle thing, TalkOrigins has the <a href="http://www.talkorigins.org/faqs/bombardier.html">full story for that</a>, too.<br><br>Fortunately, at the decision making level here in Australia, this part of the debate seems to be coming to a close, with even Toby Abbott (the leader of the opposition) at least paying lip service to the fact that human-induced increases in average global temperatures are now a fact of life. However, the mainstream media is still happy to "teach the controversy" in the hopes of picking up a few more eyeballs (or ears) to sell to their advertisers.<br><br><b>But the problem is so much bigger than us!</b><br>However, even once you get agreement that human-induced global warming due to excessive carbon dioxide emissions is a genuine problem, you then run into the "But we can't do anything about it!" attitude.<br><br>Many Australians (including some elected members of our Federal parliament) go "Oh, but the US/Europe/Brazil/Russia/India/China have a much bigger impact than we do. Doing anything will hurt our international competitiveness without really achieving anything, so we shouldn't bother."<br><br>Even the higher emission countries point fingers at each other. The US won't budge until China does. The BRIC countries are waiting for the US to make a move, and use US inaction as justification for similarly doing nothing in the meantime.<br><br>An absolutely textbook "Tragedy of the Commons" reaction. And we know how that story ends, don't we? In the long run everybody loses, nobody wins.<br><br>How do you fix it? Rules, regulations and social pressure. The "community of nations" isn't just words. The complex web of interdependencies that spans the world gives countries real power to exert influence on each other. Once some countries start to make a move towards serious carbon emission control strategies, then that becomes a bargaining chip to use in international negotiations. Will it work? Who knows. The only thing we know for sure is that the more carbon we pump into the atmosphere over the next few decades, the higher the impact from global warming will be (it's now guaranteed that there will be <i>some</i> impact - the carbon cycle is too long for us to prevent that at this late stage of the game).<br><br>Ideally you would want to adopt an emissions trading scheme along the lines of that used to curb sulfur emissions, but the wholehearted embrace of dodgy pseudoscience by far too many members of our Federal Opposition party spiked that particular barrel.<br><br>So a carbon tax it is (for now, anyway). The hysterical cries of "Oh my god, you've doomed the country!" ring loudly from those that will bear most of the direct costs of actually internalising the negative effects their carbon emissions have on the wider environment. Their motives are, to say the least, a little bit suspect. The political opportunism involved in our Federal Opposition leader backing them is disappointing, but unsurprising.<br><br><b>In the PM's words</b><br>There's a nice summary of the current situation in a Fairfax <a href="http://www.smh.com.au/opinion/politics/extreme-views-must-not-decide-carbon-tax-20110323-1c6ky.html">opinion piece</a> that went out over Gillard's byline:<br><blockquote><i>The longer we leave it the harder action on climate change gets. This reform road is a hard one to walk. Just as doing nothing is not an option, we need to be careful to ensure that we do not make decisions that will cost our economy and jobs.</i></blockquote>There's a definite risk that the government's plans won't live up to their ambitions. However, the world's past experience with the sulfur dioxide doomsayers should arouse some deep skepticism and not directed towards those wanting to press forward with plans to curb carbon emissions.
            
    <p>
        <a href="posts/201103climate-change-skepticism-text-book.html#disqus_thread" data-disqus-identifier="cache/posts/201103climate-change-skepticism-text-book.html">Comments</a>

        </p></div>
    
<div>
<ul class="pager">
    <li class="previous">
        <a href="index-1.html">← Newer posts</a>
    </li><li class="next">
        <a href="index-3.html">Older posts →</a>
</li></ul>
</div>

    
       <script type="text/javascript">var disqus_shortname="boredomandlaziness";(function(){var a=document.createElement("script");a.async=true;a.type="text/javascript";a.src="http://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)}());</script>

    


    </div>
    </div>
    <!--End of body content-->
</div>
<div class="footerbox">
    Contents © 2013 <a href="mailto:ncoghlan@gmail.com">Nick Coghlan</a> - <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a>, republish as you wish. - Powered by <a href="http://nikola.ralsina.com.ar">Nikola</a>
</div>



            <script src="assets/js/jquery-1.7.2.min.js" type="text/javascript"></script>
            <script src="assets/js/bootstrap.min.js" type="text/javascript"></script>
        <script src="assets/js/jquery.colorbox-min.js" type="text/javascript"></script>


    <script type="text/javascript">jQuery("a.image-reference").colorbox({rel:"gal",maxWidth:"80%",maxHeight:"80%",scalePhotos:true});</script>
</body>
</html>